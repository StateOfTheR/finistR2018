<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>greta</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 54px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 59px;
  margin-top: -59px;
}

.section h2 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h3 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h4 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h5 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h6 {
  padding-top: 59px;
  margin-top: -59px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">FinistR 2018</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    accueil
  </a>
</li>
<li>
  <a href="https://github.com/StateOfTheR/finistR2018">
    <span class="fa fa-github"></span>
     
    dépôt github
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-refresh"></span>
     
    Webscrapping
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="atelier1_webscraping_intro.html">Introduction</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="atelier1_webscraping_simple.html">Un exemple simple</a>
    </li>
    <li>
      <a href="atelier1_webscraping_genealogie.html">Généalogie mathématique</a>
    </li>
    <li>
      <a href="atelier1_webscraping_tripadvisor.html">Trip advisor</a>
    </li>
    <li>
      <a href="atelier1_webscraping_googlefight.html">Occurrences Google</a>
    </li>
    <li>
      <a href="atelier1_webscraping_youtube.html">Youtube, MSN meteo</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-wheelchair-alt"></span>
     
    Optimisation et accélération
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="atelier2_introduction.html">Introduction</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="atelier2_background_optim.html">Outils d'optimisation standard</a>
    </li>
    <li>
      <a href="atelier2_rgpu.html">R et GPU</a>
    </li>
    <li>
      <a href="atelier2_tensorflow.html">TensorFlow</a>
    </li>
    <li>
      <a href="atelier2_greta.html">Greta</a>
    </li>
    <li>
      <a href="atelier2_rcpp_modules.html">Modules Rcpp</a>
    </li>
    <li>
      <a href="atelier2_julia.html">Julia</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-thumbs-o-up"></span>
     
    Bonnes pratiques
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction</li>
    <li class="divider"></li>
    <li>
      <a href="atelier3_git.html">Versioning</a>
    </li>
    <li>
      <a href="atelier3_package_creation.html">Packages: création, développement, tests</a>
    </li>
    <li>
      <a href="atelier3_advancedrmd.html">Production de documents</a>
    </li>
  </ul>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">greta</h1>

</div>


<div id="what-is-greta" class="section level1">
<h1>What is greta</h1>
<p>greta relies on TensorFlow and use efficient inference algorithms like Hamiltonian Monte Carlo. The main objective of greta is to provide a MCMC software like BUGS, JAGS or Stan. We can find on greta’s <a href="https://greta-dev.github.io/greta/index.html">website</a> the following definition:</p>
<ul>
<li><em>simple : greta models are written right in R, so there’s no need to learn another language like BUGS or Stan</em></li>
<li><em>scalable : greta uses Google TensorFlow so it’s fast even on massive datasets, and runs on CPU clusters and GPUs</em></li>
<li><em>extensible : it’s easy to write your own R functions, modules, and even packages using greta</em></li>
</ul>
<p>The main advantage of greta is it builds statistical models interactively in R, and then sample from them by MCMC. There is no need to create a model object like in JAGS, Stan or nimble. However, greta creates specific objects (i.e. greta arrays) which are used to build the model and define the data to be used in the inference. greta is still is in its early development (version 0.2.3 on CRAN) and at this time there is a lack of documentation (especially compared with Stan, JAGS or nimble). We chose to test greta on a very easy and standard exemple. We further compare greta performances with other Bayesian inference tools : Jags, Nimble and Stan.</p>
<div id="hmc" class="section level2">
<h2>HMC</h2>
<p>For now, greta is only using the Hamiltonian Monte Carlo algorithm, a most remarkable Markov chain Monte Carlo method for Bayesian inference. HMC reduces the correlation between successive sampled states by using properties from Hamiltonian dynamics <a href="https://www.cs.toronto.edu/~radford/ftp/ham-mcmc.pdf">see the nice paper from R.M. Neal on the topic</a>. It allows larger moves between states at the cost of doubling the dimension of the state space and being able to efficiently compute the gradient of the logposterior density. This causes such an algorithm to converge more quickly to the targeted posterior probability distribution. There is a growing interest to implement HMC algorithms in the many Bayesian toolboxes.</p>
</div>
<div id="installation" class="section level2">
<h2>installation</h2>
<p>greta use TensorFlow and it needs to be installed before installing greta.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">&#39;tensorflow&#39;</span>)
<span class="kw">install_tensorflow</span>()</code></pre></div>
<p>greta also uses the DiagrammeR R package for plotting Directed Acyclic Graph of greta models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">&#39;greta&#39;</span>)
<span class="kw">install.packages</span>(<span class="st">&#39;DiagrammeR&#39;</span>)</code></pre></div>
</div>
<div id="how-it-works" class="section level2">
<h2>How it works</h2>
<p>On greta’s website a getting started <a href="https://greta-dev.github.io/greta/get_started.html">page</a> as well as <a href="https://greta-dev.github.io/greta/example_models.html">examples</a> are available. In the following section, we use greta with a simple dynamic model, for which there is no example on greta’s website.</p>
</div>
</div>
<div id="gaussian-state-space-model-with-greta" class="section level1">
<h1>Gaussian State Space model with greta</h1>
<p>The latent process <span class="math inline">\(X\)</span> is assumed to be an independent random walk, which is a cumulative sum of a sequence of independent process errors <span class="math inline">\(\epsilon_x\)</span>.</p>
<p><span class="math display">\[X_{t} = \rho_x X_{t-1} + \epsilon_x \quad \text{with} \quad \epsilon_x \sim Normal(0, \sigma_x) \]</span> The observation process <span class="math inline">\(Y\)</span> is defined as: <span class="math display">\[Y_{t} = \rho_y X_{t} + \epsilon_y \quad \text{with} \quad \epsilon_y \sim Normal(0, \sigma_y) \]</span></p>
<div id="greta-in-practice" class="section level2">
<h2>greta in practice</h2>
<div id="packages-loading" class="section level3">
<h3>packages loading</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&#39;greta&#39;</span>)
<span class="kw">library</span>(<span class="st">&#39;MASS&#39;</span>)
<span class="kw">library</span>(<span class="st">&#39;tidyverse&#39;</span>)
<span class="kw">library</span>(<span class="st">&#39;DiagrammeR&#39;</span>)</code></pre></div>
</div>
<div id="data-simulation" class="section level3">
<h3>Data simulation</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)

N &lt;-<span class="st"> </span><span class="dv">20</span> <span class="co"># number of time step</span>
x_true &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, N) <span class="co"># latent process</span>
y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, N) <span class="co"># observations</span>

<span class="co"># parameters</span>
sd_x_true &lt;-<span class="st"> </span><span class="dv">1</span>
prec_x_init_true &lt;-<span class="st"> </span>sd_x_true<span class="op">^</span>(<span class="op">-</span><span class="dv">2</span>)
sd_y_true &lt;-<span class="st"> </span><span class="dv">1</span>
prec_y_true &lt;-<span class="st"> </span>sd_y_true<span class="op">^</span>(<span class="op">-</span><span class="dv">2</span>)
mean_x_init_true &lt;-<span class="st"> </span><span class="dv">0</span>
r_x_true &lt;-<span class="st"> </span><span class="fl">0.85</span>
r_y_true &lt;-<span class="st"> </span><span class="dv">1</span>

### Process and data generation
x_true[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, mean_x_init_true, sd_x_true)

y[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, r_y_true<span class="op">*</span>x_true[<span class="dv">1</span>], sd_y_true)

<span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>N){
  x_true[t] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, r_x_true<span class="op">*</span>x_true[t<span class="op">-</span><span class="dv">1</span>], sd_x_true)
  y[t] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, r_y_true<span class="op">*</span>x_true[t], sd_y_true)
}</code></pre></div>
</div>
<div id="greta-data-and-priors" class="section level3">
<h3>greta data and priors</h3>
<p>The first step is to declare data for greta, assign priors to unknown parameters and provide distribution to random variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># greta&#39;s data</span>
obs &lt;-<span class="st"> </span><span class="kw">as_data</span>(y)

<span class="co"># priors</span>
r_y &lt;-<span class="st"> </span><span class="kw">gamma</span>(<span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># gamma prior set to r_y</span>
sd_x &lt;-<span class="st"> </span><span class="kw">inverse_gamma</span>(<span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># inv-gamma prior set to sd_x</span>

<span class="co"># fixed parameters</span>
sd_y &lt;-<span class="st"> </span>sd_y_true <span class="co"># parameter fixed to the true value</span>
r_x &lt;-<span class="st"> </span>r_x_true <span class="co"># parameter fixed to the true value</span>

<span class="co"># creation of an empty greta array</span>
epsilon_x &lt;-<span class="st"> </span><span class="kw">normal</span>(<span class="dv">0</span>, sd_x, <span class="dt">dim =</span> N)</code></pre></div>
<p>A prior gamma distribution is assigned to <span class="math inline">\(\rho_y\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(r_y)</code></pre></div>
<pre><code>## greta array (variable following a gamma distribution)
## 
##      [,1]
## [1,]  ?</code></pre>
<p>*_x* is an empty greta array</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(epsilon_x)</code></pre></div>
<pre><code>## greta array (variable following a normal distribution)
## 
##       [,1]
##  [1,]  ?  
##  [2,]  ?  
##  [3,]  ?  
##  [4,]  ?  
##  [5,]  ?  
##  [6,]  ?  
##  [7,]  ?  
##  [8,]  ?  
##  [9,]  ?  
## [10,]  ?  
## [11,]  ?  
## [12,]  ?  
## [13,]  ?  
## [14,]  ?  
## [15,]  ?  
## [16,]  ?  
## [17,]  ?  
## [18,]  ?  
## [19,]  ?  
## [20,]  ?</code></pre>
<p>the same process is used to define the latent variable <span class="math inline">\(X\)</span> for greta</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># creation of an empty greta array for the X</span>
X &lt;-<span class="st"> </span><span class="kw">greta_array</span>(<span class="dt">data =</span> <span class="dv">0</span>, <span class="dt">dim =</span> <span class="kw">c</span>(N, <span class="dv">1</span>))
<span class="co"># definition of the latent process interpreted by greta</span>
<span class="cf">for</span>(t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){
  X[t, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">sum</span>(r_x<span class="op">^</span>((t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">-</span><span class="st"> </span>(<span class="dv">0</span><span class="op">:</span>(t<span class="op">-</span><span class="dv">1</span>))) <span class="op">*</span><span class="st"> </span>epsilon_x[<span class="dv">1</span><span class="op">:</span>t, <span class="dv">1</span>])
}</code></pre></div>
</div>
<div id="greta-model-definition" class="section level3">
<h3>greta model definition</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># definition of the distribution of the observation</span>
<span class="kw">distribution</span>(obs) &lt;-<span class="st"> </span><span class="kw">normal</span>(X <span class="op">*</span><span class="st"> </span>r_y, <span class="kw">rep</span>(sd_y, N), <span class="dt">dim =</span> N)
<span class="co"># greta&#39;s model building</span>
model_greta &lt;-<span class="st"> </span><span class="kw">model</span>(X, r_y, sd_x, <span class="dt">precision =</span> <span class="st">&#39;double&#39;</span>)</code></pre></div>
<p>In this case study, greta faces numerical instability. The option <em>precision = ’double</em> of the model greta function redefines the model to have double precision and increase numerical stability, though this will slow down sampling. Once, the greta’s model is built, greta in combination with the DiagrammeR package can generate a DAG representing the greta’s model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(model_greta)</code></pre></div>
<div class="figure">
<img src="img/DAG.png" alt="DAG generated by greta" />
<p class="caption">DAG generated by greta</p>
</div>
</div>
</div>
<div id="greta-model-inference" class="section level2">
<h2>greta model inference</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># run sampler for 1000 iterations</span>
draws &lt;-<span class="st"> </span>greta<span class="op">::</span><span class="kw">mcmc</span>(model_greta, <span class="dt">n_samples =</span> <span class="dv">1000</span>)</code></pre></div>
</div>
<div id="greta-model-results" class="section level2">
<h2>greta model results</h2>
<p>Results of draws are returned as an mcmc.list object and can be summarized with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print summary of mcmc.list</span>
<span class="kw">summary</span>(draws)</code></pre></div>
<pre><code>## 
## Iterations = 1:1000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 1000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##         Mean     SD Naive SE Time-series SE
## X1    0.3733 2.1462  0.06787        0.15772
## X2    3.0768 3.6037  0.11396        1.25844
## X3    4.4626 4.1282  0.13055        1.07753
## X4    2.4568 3.1616  0.09998        0.35033
## X5    2.2760 3.1442  0.09943        0.33303
## X6    5.1728 5.0974  0.16119        1.45225
## X7    5.1343 4.4484  0.14067        0.93521
## X8    5.5734 5.0864  0.16085        1.63427
## X9    2.1392 3.1725  0.10032        0.30437
## X10   1.8550 2.7917  0.08828        0.21037
## X11   0.2762 2.6706  0.08445        0.19031
## X12  -2.8937 3.6834  0.11648        0.72436
## X13  -3.9724 4.0258  0.12731        0.70530
## X14  -1.7496 2.9809  0.09426        0.45316
## X15  -0.9563 3.2676  0.10333        0.25173
## X16  -0.6495 2.9168  0.09224        0.37437
## X17   2.4116 3.1900  0.10088        0.46407
## X18   3.6987 3.8315  0.12116        0.50926
## X19   3.8339 3.7879  0.11979        0.71358
## X20   2.7806 4.0137  0.12692        0.78119
## r_y   0.4706 0.3661  0.01158        0.05673
## sd_x  2.7788 2.1287  0.06731        0.80756
## 
## 2. Quantiles for each variable:
## 
##           2.5%     25%     50%      75%   97.5%
## X1    -4.50040 -0.6228  0.2112  1.31390  5.4583
## X2    -0.81292  0.7749  1.7493  4.29892 13.3899
## X3     0.15744  1.5189  3.1242  6.22269 15.6027
## X4    -1.86488  0.5150  1.6604  3.62041 10.9946
## X5    -3.10004  0.6474  1.5933  3.46203 10.3607
## X6     0.27236  1.7645  3.3061  7.13861 19.4925
## X7     0.22941  1.9474  3.7629  7.09277 16.6199
## X8     0.39509  1.9411  3.7288  7.31639 18.0841
## X9    -2.22085  0.2212  1.1963  3.39244 10.6713
## X10   -2.39083  0.2736  1.1509  3.05590  8.6918
## X11   -4.90656 -0.9998  0.0721  1.37473  6.5716
## X12  -12.66844 -4.3593 -1.8073 -0.61175  1.8752
## X13  -13.88930 -5.7624 -2.5929 -1.25314  0.8173
## X14   -9.63065 -2.9333 -0.9797  0.01484  2.6153
## X15   -9.77126 -1.7322 -0.4656  0.44309  4.2267
## X16   -8.57777 -1.3820 -0.2762  0.73185  3.7284
## X17   -2.04558  0.5351  1.5252  3.39879 11.5449
## X18   -0.66031  1.1807  2.4650  5.08484 14.0150
## X19   -0.46447  1.2499  2.6144  5.57447 13.1233
## X20   -2.62928  0.6870  1.8021  3.80186 14.3480
## r_y    0.09029  0.1914  0.3643  0.62026  1.4288
## sd_x   0.59143  1.1603  1.9813  3.97007  7.7680</code></pre>
<p>Using the tidyverse, a plot reprsenting simulated data, and posterior distributions is generated</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### extract data from mcmc.list object to tibble for easy plotting with ggplot2
bounds &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(<span class="kw">t</span>(<span class="kw">apply</span>(draws[[<span class="dv">1</span>]][, <span class="op">-</span><span class="kw">c</span>(<span class="dv">21</span>, <span class="dv">22</span>)],<span class="dv">2</span>,<span class="cf">function</span>(x) <span class="kw">quantile</span>(x,<span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.5</span>,<span class="fl">0.975</span>)))))
bounds<span class="op">$</span>param &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N
<span class="kw">colnames</span>(bounds)[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;min&quot;</span>,<span class="st">&quot;med&quot;</span>,<span class="st">&quot;max&quot;</span>)

ggdraws &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(draws[[<span class="dv">1</span>]][, <span class="op">-</span><span class="kw">c</span>(<span class="dv">21</span>, <span class="dv">22</span>)]) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(param, value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">param =</span> <span class="kw">as.numeric</span>(<span class="kw">str_extract</span>(param, <span class="st">&quot;[0-9]+&quot;</span>)))

true &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(<span class="kw">data.frame</span>(<span class="dt">X=</span>x_true,<span class="dt">param=</span><span class="dv">1</span><span class="op">:</span><span class="dv">20</span>))
obs &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(<span class="kw">data.frame</span>(<span class="dt">Y=</span>y,<span class="dt">param=</span><span class="dv">1</span><span class="op">:</span><span class="dv">20</span>))
data &lt;-<span class="st"> </span><span class="kw">left_join</span>(ggdraws,bounds,<span class="dt">by=</span><span class="st">&quot;param&quot;</span>)
data &lt;-<span class="st"> </span><span class="kw">left_join</span>(data,true,<span class="dt">by=</span><span class="st">&quot;param&quot;</span>)
data &lt;-<span class="st"> </span><span class="kw">left_join</span>(data,obs,<span class="dt">by=</span><span class="st">&quot;param&quot;</span>)

### generating plot
<span class="kw">ggplot</span>(data, <span class="kw">aes</span>(param, value)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="fl">0.2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> min, <span class="dt">ymax =</span> max), <span class="dt">fill =</span> <span class="st">&quot;gold&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(param, X, <span class="dt">colour =</span> <span class="st">&quot;col1&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(param, Y, <span class="dt">colour =</span> <span class="st">&quot;col2&quot;</span>), <span class="dt">size =</span> <span class="fl">0.6</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(param, med, <span class="dt">colour=</span> <span class="st">&quot;col3&quot;</span>), <span class="dt">size =</span> <span class="fl">1.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">name =</span> <span class="st">&quot;&quot;</span>,
                      <span class="dt">values =</span> <span class="kw">c</span>(<span class="dt">col1 =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">col2 =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">col3 =</span> <span class="st">&quot;violetred3&quot;</span>),
                      <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;True&quot;</span>, <span class="st">&quot;Observed&quot;</span>, <span class="st">&quot;Median estimate&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span> (<span class="dt">x =</span> <span class="st">&quot;Time&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Simulated values&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Simulated data and their posterior distributions (black points).&quot;</span>, <span class="dt">subtitle =</span> <span class="st">&#39;Yellow shade represents the CI at 95%&#39;</span>)</code></pre></div>
<p><img src="atelier2_greta_files/figure-html/greta7-1.png" width="672" /></p>
</div>
<div id="comparison-with-jags-nimble-and-stan" class="section level2">
<h2>Comparison with Jags, Nimble and Stan</h2>
<div id="jags" class="section level3">
<h3>Jags</h3>
<p>Written in C++, Jags (Just another Gibbs sampler <a href="https://web.sgh.waw.pl/~atoroj/ekonometria_bayesowska/jags_user_manual.pdf">Just another Gibbs sampler</a> ) is becomming the common tool for Bayesian newbies. Conversely to WinBUGS/OpenBUGS platform (<a href="https://www.mrc-bsu.cam.ac.uk/software/bugs/">BUGS</a>) written in Pascal component and quasi-impossible to install on anything but Windows systems, Jags can be launched as a stand-alone routine from R whatever the operating system. It does not rely as greta on the recent Hamiltonian Monte algorithm to visit the space of the model’s unknowns ( and subsequently does not call Tensorflow as a quick routine to perform Hamiltonian dynamics). But Jags is known to work quite fast (since it is developped by Martyn Plummer, founding member of the Rcore team) and we’d wished to compare its performance to greta’s previous ones on this very simple linear Kalman filter example. A jag’s code treating the simple Kalman model would look like the following lines of code :</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rjags)
burnSize &lt;-<span class="st"> </span><span class="dv">100</span>
numberIter &lt;-<span class="st"> </span><span class="dv">1000</span>
thin &lt;-<span class="st"> </span><span class="dv">1</span>
params &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;x&#39;</span>, <span class="st">&#39;pres_x&#39;</span>, <span class="st">&#39;r_y&#39;</span>)

<span class="kw">require</span>(rjags)
jags.data &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">y=</span>y,<span class="dt">N=</span>N,<span class="dt">prec_x_init=</span>prec_x_init_true,<span class="dt">prec_y=</span>prec_y_true,
                 <span class="dt">mean_x_init=</span>mean_x_init_true,<span class="dt">r_x =</span> r_x_true)
init&lt;-<span class="st"> </span><span class="cf">function</span>(){
  <span class="kw">list</span>(<span class="dt">x=</span><span class="kw">rnorm</span>(N,y,<span class="fl">0.2</span>),<span class="dt">r_y=</span><span class="kw">rgamma</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">prec_x=</span><span class="kw">rgamma</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>))
}
modelstring=<span class="st">&quot;</span>
<span class="st">model{</span>

<span class="st"># Constants fixed (known) in the data</span>
<span class="st"># N &lt;- 20</span>
<span class="st"># prec_x_init &lt;- 1</span>
<span class="st"># prec_y &lt;- 1</span>
<span class="st"># mean_x_init &lt;- 0</span>
<span class="st"># r_x &lt;- 1</span>

<span class="st">prec_x ~ dgamma(1,1)</span>
<span class="st">r_y ~ dgamma(1,1)</span>
<span class="st"># r_y ~ dnorm(0,1)I(0,)</span>

<span class="st">x[1] ~ dnorm(mean_x_init, prec_x_init)</span>
<span class="st">y[1] ~ dnorm(r_y*x[1], prec_y)</span>

<span class="st">for (t in 2:N)</span>
<span class="st">{</span>
<span class="st">  x[t] ~ dnorm(r_x*x[t-1], prec_x)</span>
<span class="st">  y[t] ~ dnorm(r_y*x[t], prec_y)</span>
<span class="st">}</span>

<span class="st">}</span>
<span class="st">&quot;</span>
model=<span class="kw">jags.model</span>(<span class="kw">textConnection</span>(modelstring), <span class="dt">data=</span>jags.data, <span class="dt">inits=</span>init)</code></pre></div>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 20
##    Unobserved stochastic nodes: 22
##    Total graph size: 87
## 
## Initializing model
## 
## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |+                                                 |   2%
  |                                                        
  |++                                                |   4%
  |                                                        
  |+++                                               |   6%
  |                                                        
  |++++                                              |   8%
  |                                                        
  |+++++                                             |  10%
  |                                                        
  |++++++                                            |  12%
  |                                                        
  |+++++++                                           |  14%
  |                                                        
  |++++++++                                          |  16%
  |                                                        
  |+++++++++                                         |  18%
  |                                                        
  |++++++++++                                        |  20%
  |                                                        
  |+++++++++++                                       |  22%
  |                                                        
  |++++++++++++                                      |  24%
  |                                                        
  |+++++++++++++                                     |  26%
  |                                                        
  |++++++++++++++                                    |  28%
  |                                                        
  |+++++++++++++++                                   |  30%
  |                                                        
  |++++++++++++++++                                  |  32%
  |                                                        
  |+++++++++++++++++                                 |  34%
  |                                                        
  |++++++++++++++++++                                |  36%
  |                                                        
  |+++++++++++++++++++                               |  38%
  |                                                        
  |++++++++++++++++++++                              |  40%
  |                                                        
  |+++++++++++++++++++++                             |  42%
  |                                                        
  |++++++++++++++++++++++                            |  44%
  |                                                        
  |+++++++++++++++++++++++                           |  46%
  |                                                        
  |++++++++++++++++++++++++                          |  48%
  |                                                        
  |+++++++++++++++++++++++++                         |  50%
  |                                                        
  |++++++++++++++++++++++++++                        |  52%
  |                                                        
  |+++++++++++++++++++++++++++                       |  54%
  |                                                        
  |++++++++++++++++++++++++++++                      |  56%
  |                                                        
  |+++++++++++++++++++++++++++++                     |  58%
  |                                                        
  |++++++++++++++++++++++++++++++                    |  60%
  |                                                        
  |+++++++++++++++++++++++++++++++                   |  62%
  |                                                        
  |++++++++++++++++++++++++++++++++                  |  64%
  |                                                        
  |+++++++++++++++++++++++++++++++++                 |  66%
  |                                                        
  |++++++++++++++++++++++++++++++++++                |  68%
  |                                                        
  |+++++++++++++++++++++++++++++++++++               |  70%
  |                                                        
  |++++++++++++++++++++++++++++++++++++              |  72%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++             |  74%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++            |  76%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++           |  78%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++          |  80%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++         |  82%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++        |  84%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++       |  86%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++      |  88%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t1 &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()
<span class="kw">update</span>(model,<span class="dt">n.iter=</span>burnSize)</code></pre></div>
<pre><code>## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |*                                                 |   2%
  |                                                        
  |**                                                |   4%
  |                                                        
  |***                                               |   6%
  |                                                        
  |****                                              |   8%
  |                                                        
  |*****                                             |  10%
  |                                                        
  |******                                            |  12%
  |                                                        
  |*******                                           |  14%
  |                                                        
  |********                                          |  16%
  |                                                        
  |*********                                         |  18%
  |                                                        
  |**********                                        |  20%
  |                                                        
  |***********                                       |  22%
  |                                                        
  |************                                      |  24%
  |                                                        
  |*************                                     |  26%
  |                                                        
  |**************                                    |  28%
  |                                                        
  |***************                                   |  30%
  |                                                        
  |****************                                  |  32%
  |                                                        
  |*****************                                 |  34%
  |                                                        
  |******************                                |  36%
  |                                                        
  |*******************                               |  38%
  |                                                        
  |********************                              |  40%
  |                                                        
  |*********************                             |  42%
  |                                                        
  |**********************                            |  44%
  |                                                        
  |***********************                           |  46%
  |                                                        
  |************************                          |  48%
  |                                                        
  |*************************                         |  50%
  |                                                        
  |**************************                        |  52%
  |                                                        
  |***************************                       |  54%
  |                                                        
  |****************************                      |  56%
  |                                                        
  |*****************************                     |  58%
  |                                                        
  |******************************                    |  60%
  |                                                        
  |*******************************                   |  62%
  |                                                        
  |********************************                  |  64%
  |                                                        
  |*********************************                 |  66%
  |                                                        
  |**********************************                |  68%
  |                                                        
  |***********************************               |  70%
  |                                                        
  |************************************              |  72%
  |                                                        
  |*************************************             |  74%
  |                                                        
  |**************************************            |  76%
  |                                                        
  |***************************************           |  78%
  |                                                        
  |****************************************          |  80%
  |                                                        
  |*****************************************         |  82%
  |                                                        
  |******************************************        |  84%
  |                                                        
  |*******************************************       |  86%
  |                                                        
  |********************************************      |  88%
  |                                                        
  |*********************************************     |  90%
  |                                                        
  |**********************************************    |  92%
  |                                                        
  |***********************************************   |  94%
  |                                                        
  |************************************************  |  96%
  |                                                        
  |************************************************* |  98%
  |                                                        
  |**************************************************| 100%</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out=<span class="kw">coda.samples</span>(<span class="dt">model=</span>model,<span class="dt">variable.names=</span>params, <span class="dt">n.iter=</span>numberIter, <span class="dt">thin=</span>thin)</code></pre></div>
<pre><code>## Warning in jags.samples(model, variable.names, n.iter, thin, type = &quot;trace&quot;, : Failed to set trace monitor for pres_x
## Variable pres_x not found</code></pre>
<pre><code>## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |*                                                 |   2%
  |                                                        
  |**                                                |   4%
  |                                                        
  |***                                               |   6%
  |                                                        
  |****                                              |   8%
  |                                                        
  |*****                                             |  10%
  |                                                        
  |******                                            |  12%
  |                                                        
  |*******                                           |  14%
  |                                                        
  |********                                          |  16%
  |                                                        
  |*********                                         |  18%
  |                                                        
  |**********                                        |  20%
  |                                                        
  |***********                                       |  22%
  |                                                        
  |************                                      |  24%
  |                                                        
  |*************                                     |  26%
  |                                                        
  |**************                                    |  28%
  |                                                        
  |***************                                   |  30%
  |                                                        
  |****************                                  |  32%
  |                                                        
  |*****************                                 |  34%
  |                                                        
  |******************                                |  36%
  |                                                        
  |*******************                               |  38%
  |                                                        
  |********************                              |  40%
  |                                                        
  |*********************                             |  42%
  |                                                        
  |**********************                            |  44%
  |                                                        
  |***********************                           |  46%
  |                                                        
  |************************                          |  48%
  |                                                        
  |*************************                         |  50%
  |                                                        
  |**************************                        |  52%
  |                                                        
  |***************************                       |  54%
  |                                                        
  |****************************                      |  56%
  |                                                        
  |*****************************                     |  58%
  |                                                        
  |******************************                    |  60%
  |                                                        
  |*******************************                   |  62%
  |                                                        
  |********************************                  |  64%
  |                                                        
  |*********************************                 |  66%
  |                                                        
  |**********************************                |  68%
  |                                                        
  |***********************************               |  70%
  |                                                        
  |************************************              |  72%
  |                                                        
  |*************************************             |  74%
  |                                                        
  |**************************************            |  76%
  |                                                        
  |***************************************           |  78%
  |                                                        
  |****************************************          |  80%
  |                                                        
  |*****************************************         |  82%
  |                                                        
  |******************************************        |  84%
  |                                                        
  |*******************************************       |  86%
  |                                                        
  |********************************************      |  88%
  |                                                        
  |*********************************************     |  90%
  |                                                        
  |**********************************************    |  92%
  |                                                        
  |***********************************************   |  94%
  |                                                        
  |************************************************  |  96%
  |                                                        
  |************************************************* |  98%
  |                                                        
  |**************************************************| 100%</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tJags &lt;-<span class="st"> </span><span class="kw">Sys.time</span>() <span class="op">-</span><span class="st"> </span>t1
<span class="kw">print</span>(<span class="kw">summary</span>(out))</code></pre></div>
<pre><code>## 
## Iterations = 1101:2100
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 1000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##            Mean     SD Naive SE Time-series SE
## r_y    0.575813 0.3362  0.01063        0.05843
## x[1]   0.136954 0.8477  0.02681        0.03449
## x[2]   1.844584 1.9550  0.06182        0.23155
## x[3]   3.047286 2.7198  0.08601        0.56005
## x[4]   1.897944 2.1687  0.06858        0.36966
## x[5]   1.848046 2.2865  0.07230        0.30554
## x[6]   2.986937 2.5870  0.08181        0.52525
## x[7]   3.317481 2.8487  0.09008        0.65734
## x[8]   3.247659 3.0651  0.09693        0.76437
## x[9]   1.191080 1.9371  0.06126        0.22303
## x[10]  1.220380 1.8785  0.05940        0.18263
## x[11] -0.006087 1.7566  0.05555        0.07095
## x[12] -1.667764 2.1573  0.06822        0.19640
## x[13] -2.665847 2.7907  0.08825        0.65235
## x[14] -1.099115 1.8673  0.05905        0.22544
## x[15] -0.289025 1.5896  0.05027        0.06088
## x[16]  0.024094 1.7168  0.05429        0.10018
## x[17]  1.791516 2.1153  0.06689        0.28013
## x[18]  2.720068 2.6158  0.08272        0.56445
## x[19]  2.549954 2.4543  0.07761        0.54337
## x[20]  1.838128 2.2505  0.07117        0.30201
## 
## 2. Quantiles for each variable:
## 
##            2.5%     25%      50%      75%  97.5%
## r_y     0.11018  0.3275  0.50881  0.77756  1.478
## x[1]   -1.62338 -0.3670  0.13406  0.67397  1.831
## x[2]   -0.79352  0.6836  1.46025  2.45338  7.005
## x[3]    0.06797  1.4563  2.35008  3.68132 10.404
## x[4]   -1.00550  0.6432  1.46407  2.60132  7.678
## x[5]   -1.31979  0.6135  1.45400  2.65907  7.725
## x[6]   -0.22091  1.3760  2.43954  3.76556 10.085
## x[7]   -0.03489  1.6774  2.55773  4.04184 11.947
## x[8]   -0.16708  1.5592  2.52598  3.90856 13.016
## x[9]   -1.53206  0.1167  0.91631  1.88513  6.697
## x[10]  -1.64896  0.2192  0.87145  1.86952  6.046
## x[11]  -3.38408 -0.7270  0.02327  0.73038  3.788
## x[12]  -7.19012 -2.2485 -1.21246 -0.47906  1.342
## x[13] -10.84295 -3.3668 -1.87924 -1.10729  0.650
## x[14]  -5.75139 -1.7380 -0.80523 -0.05479  1.790
## x[15]  -3.78828 -1.0519 -0.25874  0.51900  2.891
## x[16]  -3.44545 -0.7934  0.02308  0.80429  3.555
## x[17]  -0.99351  0.5670  1.29617  2.49821  7.213
## x[18]  -0.16358  1.1600  2.00201  3.35278 10.145
## x[19]  -0.24026  1.0915  1.94641  3.30875  9.140
## x[20]  -1.02330  0.5592  1.38766  2.53240  7.336</code></pre>
</div>
<div id="nimble" class="section level3">
<h3>Nimble</h3>
<p><a href="https://r-nimble.org">NIMBLE</a> (Numerical Inference for Statistical Models using Bayesian and Likelihood Estimation) is an competing R package developed by Perry De Valpine, Christopher Paciorek, Duncan Temple, and Daniel Turek . The package is designed to deal with hierarchical models and the many problems they raise. The NMBLE creators allow scientist to write their own blocks of the inference routine. The idea underpinning NIMBLE is to allow both a flexible model specification and a programming system that adapts to the model structure and gets compiled in C++. It recently incorporates Hamilton Monte Carlo algorithm within the many samplers that can be customized.</p>
</div>
<div id="stan" class="section level3">
<h3>Stan</h3>
<p>The authors of the R package <a href="http://mc-stan.org">Rstan</a> have developed the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Stan, now on a stabilized version, provides useful modern tools for Bayesian inference for continuous-variable models that are diffusing in a wide range of application domains.</p>
<p>The Stan package is rather tricky to implement and we do not compare its performances here.</p>
</div>
<div id="results" class="section level3">
<h3>Results</h3>
<p>The two unknown parameters of our exemple, <span class="math inline">\(\sigma_x\)</span> and <span class="math inline">\(r_y\)</span>, are learnt from the data. Bayesian inference is simply getting the posterior of this couple <span class="math inline">\(\sigma_x,r_y\)</span> and the latent variables <span class="math inline">\(X\)</span> (i.e. saying what you know given the data). The same data having been passed to all toolboxes, with the same priors defined for <span class="math inline">\((\sigma_x,r_y)\)</span>, the obtained posteriors should not differ in theory. As they are given as samples, one can plot the marginal distributions of each quantities with the true instances that were used to generate the data and see that their posteriors look quite the same, as expected.</p>
<p><img src="img/postSDX.png" /> <img src="img/postRY.png" /> However, MCMC algorithms yield different correlations in the sample of posterior values generated by the competing methods, which matters for the numerical precision of the estimates that can be derived from the various Monte Carlo Markov chains. The effective sample size (which can be computed using the coda package) is the size of an independent sample that would achieve the same precision. The following figure shows that greta provides the best effective sample size for all components of <span class="math inline">\(X\)</span> and the two parameters <span class="math inline">\(\sigma_x\)</span> and <span class="math inline">\(r_y\)</span>.</p>
<div class="figure">
<img src="img/effectiveSize.png" />

</div>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>greta allows to perform inference with Hamiltonian Monte Carlo algorithm like Stan or nimble. Currently, the only implemented MCMC procedure in greta is static Hamiltonian Monte Carlo. In regards of our case study, greta give similar results than JAGS and nimble. However, its early version and the small amount of documentation do not allow (yet) the implementation of complex model. greta seems promising and its development is interesting to follow for users of MCMC software.</p>
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
<p><a href="https://greta-dev.github.io/greta/index.html">greta on GitHub</a></p>
<p><a href="https://mdscheuerell.github.io/gretaDFA/#background">A more sophisticated example with latent factors</a></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
