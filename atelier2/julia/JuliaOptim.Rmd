[**Optim.jl** : Univariate and multivariate optimization in Julia](https://github.com/JuliaNLSolvers/Optim.jl/blob/master/docs/src/user/minimization.md)

```{julia}
using Pkg
Pkg.add("Optim")
Pkg.add("LineSearches")
```

```{julia}
using Optim, LineSearches
```

```{julia}
rosenbrock(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2
result = optimize(rosenbrock, zeros(2), BFGS())
```

```{julia}
 optimize(rosenbrock, zeros(2), BFGS(linesearch = LineSearches.BackTracking()))
```

```{julia}
optimize(rosenbrock, zeros(2), BFGS(linesearch = LineSearches.BackTracking(order = 2)))
```

For the Rosenbrock example, the analytical gradient can be shown to be:

```{julia}
function g!(G, x)
G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]
G[2] = 200.0 * (x[2] - x[1]^2)
end
```

```{julia}
optimize(rosenbrock, g!, zeros(2), LBFGS())
```

In addition to providing gradients, you can provide a Hessian function h! as well. In our current case this is:

```{julia}
function h!(H, x)
    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2
    H[1, 2] = -400.0 * x[1]
    H[2, 1] = -400.0 * x[1]
    H[2, 2] = 200.0
end
```

```{julia}
optimize(rosenbrock, g!, h!, zeros(2))  # newton
```
