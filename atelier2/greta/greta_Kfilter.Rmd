---
title: "Gaussian State Space model with greta"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Gaussian State Space model

$$X_{t} = \rho_x X_{t-1} + \epsilon_x \quad \text{with} \quad \epsilon_x \sim Normal(0, \sigma_x) $$

$$Y_{t} = \rho_y X_{t} + \epsilon_y \quad \text{with} \quad \epsilon_y \sim Normal(0, \sigma_y) $$


```{r packages, echo = FALSE, cache = FALSE, message = FALSE}
library(greta)
library(MASS)
library (bayesplot)
```

## greta definition from greta's website
simple : greta models are written right in R, so there’s no need to learn another language like BUGS or Stan

scalable : greta uses Google TensorFlow so it’s fast even on massive datasets, and runs on CPU clusters and GPUs

extensible : it’s easy to write your own R functions, modules, and even packages using greta

## Data simulation
```{r simulation}
set.seed(123)

## Données
N = 20; x_true = y = rep(NA,N);

sd_x_true = sd_y_true = 1
mean_x_init_true = 0
r_x_true = .85; r_y_true = 1

x_true[1] <- rnorm(1, mean_x_init_true, sd_x_true)

y[1] <- rnorm(1, r_y_true*x_true[1], sd_y_true)

for (t in 2:N){
  x_true[t] <- rnorm(1, r_x_true*x_true[t-1], sd_x_true)
  y[t] <- rnorm(1, r_y_true*x_true[t], sd_y_true)
}
```

## greta data and priors


```{r greta1}

# greta's data 
obs <- as_data(y)

# variables and priors
sd_y <- sd_y_true
r_y <- gamma(1, 1)

sd_x <- inverse_gamma(1, 1)
r_x <- r_x_true

innovations <- normal(0, sd_x, dim = N)
print(innovations)
```

```{r greta1b}

# greta's data 
obs <- as_data(y)

# variables and priors
sd_y <- sd_y_true
r_y <- gamma(1, 1)

sd_x <- inverse_gamma(1, 1)
r_x <- r_x_true

innovations <- normal(0, sd_x, dim = N)

latentState <- greta_array(data = 0, dim = c(N, 1))
for(t in 1:N){
  latentState[t, 1] <- sum(r_x^((t - 1) - (0:(t-1))) * innovations[1:t, 1])
}
```

## greta model definition
```{r greta2}
distribution(obs) <- normal(latentState * r_y, rep(sd_y, N), dim = N)
model_greta <- model(latentState, r_y, sd_x, precision = 'double')
```


## greta model DAG
```{r greta3, eval = FALSE}
plot(model_greta)
```
![DAG generated by greta](DAG.png)


## greta model DAG
```{r greta4a, echo = FALSE, eval = TRUE, include = FALSE}
if(file.exists('greta_draws.RData')){
} else {
  draws <- mcmc(model_greta, n_samples = 1000)
  save(draws, file = 'greta_draws.RData')
}
```

```{r greta4, echo = TRUE, eval = FALSE}
draws <- mcmc(model_greta, n_samples = 1000)
```

## greta model DAG
```{r greta6, echo = TRUE, include = FALSE, eval = TRUE}
load('greta_draws.RData')
```


```{r greta7}
mcmc_trace(draws)
# mcmc_intervals(draws)
```


```{r greta7b, eval = TRUE, message = FALSE}
library(tidyverse)
nlignes <- nrow(draws[[1]])
bounds <- as_tibble(t(apply(draws[[1]][, -c(21, 22)],2,function(x) quantile(x,c(0.025,0.5,0.975)))))
bounds$param <- 1:N
colnames(bounds)[1:3] <- c("min","med","max")

ggdraws <- as_tibble(draws[[1]][, -c(21, 22)]) %>%
  gather(param, value) %>%
  mutate(param = as.numeric(str_extract(param, "[0-9]+")))

true <- as_tibble(data.frame(X=x_true,param=1:20))
obs <- as_tibble(data.frame(Y=y,param=1:20))
data <- left_join(ggdraws,bounds,by="param")
data <- left_join(data,true,by="param")
data <- left_join(data,obs,by="param")


ggplot(data,aes(param,value))+
  geom_point(size=0.2)+
  geom_ribbon(aes(ymin=min,ymax=max),fill="gold",alpha=0.2)+
  geom_line(aes(param,X,colour="col1"),size=1)+
  geom_line(aes(param,Y,colour="col2"),size=0.6, linetype="dashed")+
  geom_line(aes(param,med,colour="col3"),size=1.1)+
  scale_colour_manual(name = "",
                      values = c(col1 = "blue",col2 = "black",col3 = "violetred3"),
                      labels = c("True","Observed","Median estimate"))+
  labs (x="Time",y="Simulated values")+
  theme_bw()
```



## Comparison with Jags, Nimble and Stan

### HMC
Hamiltonian Monte Carlo algorithm is a most remarkable Markov chain Monte Carlo method for Bayesian inference that reduces the correlation between successive sampled states by using properties from Hamiltonian dynamics  [see the nice paper from R.M. Neal on the topic](https://www.cs.toronto.edu/~radford/ftp/ham-mcmc.pdf). It allows larger moves between states at the cost of doubling the dimension of the state space and being able to efficiently compute the gradient of the logposterior density. This causes such an algorithm to converge more quickly to the targeted posterior probability distribution. There is a growing interest to implement HMC algorithms in the many Bayesian toolboxes. In what follows
we chose to test greta on a very easy and standard exemple. We further compare greta performances with other Bayesian inference tools : Jags, Nimble and Stan.

### Jags

Written in C++, Jags (Just another Gibbs sampler 
[Just another Gibbs sampler](https://web.sgh.waw.pl/~atoroj/ekonometria_bayesowska/jags_user_manual.pdf) ) is becomming the common tool for Bayesian newbies. Conversely to WinBUGS/OpenBUGS platform ([BUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs/)) written in Pascal component and quasi-impossible to install on anything but Windows systems, Jags can be launched as a stand-alone routine from R whatever the operating system. It does not rely as greta on the recent Hamiltonian Monte algorithm to visit the space of the model's unknowns ( and subsequently does not call Tensorflow as a quick routine to perform Hamiltonian dynamics). But Jags is known to work quite fast (since it is developped by Martyn Plummer, founding member of the Rcore team) and we'd wished to compare its performance to greta's previous ones on this very simple linear Kalman filter example.  A jag's code treating the simple Kalman model would look like the following lines of code :
```{r jagsexample}
require(rjags)
jags.data <- list(y=y,N=N,prec_x_init=prec_x_init_true,prec_y=prec_y_true, 
                 mean_x_init=mean_x_init_true,r_x = r_x_true)
init<- function(){
  list(x=rnorm(N,y,0.2),r_y=rgamma(1,1,1),prec_x=rgamma(1,1,1))
}
modelstring="
model{
    
# Constants fixed (known) in the data
# N <- 20
# prec_x_init <- 1
# prec_y <- 1
# mean_x_init <- 0
# r_x <- 1

prec_x ~ dgamma(1,1)
r_y ~ dgamma(1,1)
# r_y ~ dnorm(0,1)I(0,)	

x[1] ~ dnorm(mean_x_init, prec_x_init)
y[1] ~ dnorm(r_y*x[1], prec_y)

for (t in 2:N)
{
  x[t] ~ dnorm(r_x*x[t-1], prec_x)
  y[t] ~ dnorm(r_y*x[t], prec_y)
}

}
"
model=jags.model(textConnection(modelstring), data=jags.data, inits=init)
t1 <- Sys.time()
update(model,n.iter=burnSize)
out=coda.samples(model=model,variable.names=params, n.iter=numberIter, thin=thin)
tJags <- Sys.time() - t1
print(summary(out))
```

### Nimble

[NIMBLE](https://r-nimble.org) (Numerical Inference for Statistical Models using Bayesian and Likelihood Estimation) is an competing  R package developed by Perry De Valpine, Christopher Paciorek, Duncan Temple, and Daniel Turek . The package is designed to deal with hierarchical models and the many problems they raise. The NMBLE creators allow scientist to write their own blocks of the inference routine. The idea underpinning NIMBLE is to allow both a flexible model specification and a programming system that adapts to the model structure and gets compiled in C++. It recently incorporates Hamilton Monte Carlo algorithm within the many samplers that can be customized.


### Stan
The authors of the R package [Rstan](http://mc-stan.org) have developed the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling.  Stan, now on a stabilized version, provides useful modern tools for Bayesian inference for continuous-variable models that are diffusing in a wide range of application domains. 

The Stan package is rather tricky to implement and we do not compare its performances here.

### Results 

The two unknown parameters of our exemple, $\sigma_x$ and $r_y$, are learnt from the data. Bayesian inference is simply getting the posterior of this couple $\sigma_x,r_y$  and the latent variables $X$ (i.e. saying what you know given the data). The same data having been passed to all toolboxes, with the same priors defined for $(\sigma_x,r_y)$, the obtained posteriors should not differ in theory. As they are given as samples, one can plot the marginal distributions of each quantities with the true instances that were used to generate the data and see that their posteriors look quite the same, as expected.

![](postSDX.png)
![](postRY.png)
However, MCMC algorithms yield different correlations in the sample of posterior values generated by the competing methods, which matters for the numerical precision of the estimates that can be derived from the various Monte Carlo Markov chains.  The effective sample size (which can be computed using the coda package) is the size of an independent sample that would achieve the same precision. The following figure shows that greta provides the best effective sample size for all components of $X$ and the two parameters $\sigma_x$ and $r_y$.


![](effectiveSize.png)

## Reference 
[greta on GitHub](https://greta-dev.github.io/greta/index.html)


[A more sophisticated example with latent factors](https://mdscheuerell.github.io/gretaDFA/#background)


